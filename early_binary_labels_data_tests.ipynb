{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.load_data import load_CIFAR_data, load_Google_Digit_Data, load_MNIST_data, shuffle_data\n",
    "from functions.attacks import attackTestSet, attackTestSetBatch\n",
    "from functions.models import simple_relu_model, simple_tropical_model\n",
    "from custom_layers.initializers import BimodalBinaryInitializer, BimodalNormalInitializer, Triangular\n",
    "from tensorflow.keras import losses, initializers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test to see if adding ReLU trained weights before fitting might be a good initializer: \n",
    "#### nothing major found, but could come back to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_layers.tropLayers import TropEmbedMaxMin\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "pre_trained_weights1 = [model_objects[0].layers[0].get_weights()[0].T]\n",
    "model = Sequential([TropEmbedMaxMin(100, x_train.shape[1], initializer_w = initializers.random_normal, lam=0.01),\n",
    "                    Dense(1, activation='sigmoid',  kernel_initializer=initializers.random_normal)])\n",
    "model.get_layer('trop_embed_max_min_10').set_weights(pre_trained_weights1)\n",
    "model.compile(optimizer=Adam(0.1),loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, verbose=1)\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "plt.hist(pre_trained_weights1[0].flatten(), bins = 1000, alpha=0.2)\n",
    "plt.hist(model.layers[0].get_weights()[0].flatten(), bins = 1000, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR 2 class, accuracy look as change layer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = load_CIFAR_data(desired_classes=[7, 8])\n",
    "\n",
    "for i in range(1, 200, 2):\n",
    "    print(f'================\\n\\t{i}\\n================')\n",
    "    for j in range(5):\n",
    "        x_train, x_test, y_train, y_test = shuffle_data(x_train, x_test, y_train, y_test)\n",
    "        data = []\n",
    "        built_model = simple_tropical_model(x_train, y_train,  first_layer_size = i,  verbose = 1, lam=0.025)\n",
    "        pre_loss, pre_acc = built_model.evaluate(x_test, y_test) \n",
    "        with open('CIFAR_layer_size_test.csv', 'a', newline='') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow([i,pre_loss, pre_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scat_size = 10\n",
    "bin_num = 784\n",
    "hist_wid = 20\n",
    "hist_hei = 8\n",
    "data = pd.read_csv(\"CIFAR_layer_size_test.csv\")\n",
    "rowNum = data.shape[0]\n",
    "i_avg = data.groupby('i').mean().reset_index()\n",
    "print(i_avg)\n",
    "\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "plt.boxplot([data['accuracy'][data['i'] == i] for i in data['i'].unique()])\n",
    "plt.xlabel('layer size')\n",
    "plt.ylabel('loss')\n",
    "plt.title(f'loss from varying layer size for {rowNum} tests')\n",
    "plt.grid(True)\n",
    "plt.show() \n",
    "\n",
    "plt.scatter(data['i'], data['accuracy'], s=scat_size)\n",
    "plt.xlabel('layer size')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title(f'accuracy from varying layer size for {rowNum} tests')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(data['i'], data['loss'], s=scat_size)\n",
    "plt.xlabel('layer size')\n",
    "plt.ylabel('loss')\n",
    "plt.title(f'loss from varying layer size for {rowNum} tests')\n",
    "plt.grid(True)\n",
    "plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR 2 class test, ensembling multiple 1 layer models to test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = load_CIFAR_data(desired_classes=[7, 8])\n",
    "model_objects = []\n",
    "for i in range(5):\n",
    "    print(f'================\\n\\t{i}\\n================')\n",
    "    x_train, x_test, y_train, y_test = shuffle_data(x_train, x_test, y_train, y_test)\n",
    "    data = []\n",
    "    built_model = buildTropicalModel(x_train, y_train,  first_layer_size = 20,  verbose = 1, lam=0.025)\n",
    "    pre_loss, pre_acc = built_model.evaluate(x_test, y_test) \n",
    "    model_objects.append(built_model)\n",
    "    with open('CIFAR_1_neuron_layer_test.csv', 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow([i,pre_loss, pre_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = shuffle_data(x_train, x_test, y_train, y_test)\n",
    "num_models = len(model_objects)\n",
    "\n",
    "model_predictions = [model_objects[i].predict(x_test) for i in range(num_models)]\n",
    "ensemble_sum = np.sum(model_predictions, axis=0)\n",
    "\n",
    "ensemble_class_indices = np.array(ensemble_sum>(num_models/2)).astype(int).reshape(-1)\n",
    "accuracy = np.mean(y_test == ensemble_class_indices)\n",
    "print(f'Ensemble Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(model_predictions[0]>0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR 2 class, PGD attack, different Initializer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "\t0\n",
      "================\n",
      "Epoch 1/10\n",
      "170/313 [===============>..............] - ETA: 4s - loss: 1.2280 - accuracy: 0.7862"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kurtp\\OneDrive - Naval Postgraduate School\\Desktop\\NPS School Work\\Thesis\\TropicalNN\\Kurt_TropicalTests.ipynb Cell 13\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kurtp/OneDrive%20-%20Naval%20Postgraduate%20School/Desktop/NPS%20School%20Work/Thesis/TropicalNN/Kurt_TropicalTests.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m================\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m================\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kurtp/OneDrive%20-%20Naval%20Postgraduate%20School/Desktop/NPS%20School%20Work/Thesis/TropicalNN/Kurt_TropicalTests.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m data \u001b[39m=\u001b[39m []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kurtp/OneDrive%20-%20Naval%20Postgraduate%20School/Desktop/NPS%20School%20Work/Thesis/TropicalNN/Kurt_TropicalTests.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m built_model \u001b[39m=\u001b[39m simple_tropical_model(x_train, y_train, num_epochs \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, first_layer_size \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m, initializer_w\u001b[39m=\u001b[39;49minitializers\u001b[39m.\u001b[39;49mRandomNormal(mean\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m, stddev\u001b[39m=\u001b[39;49m\u001b[39m0.005\u001b[39;49m, seed\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kurtp/OneDrive%20-%20Naval%20Postgraduate%20School/Desktop/NPS%20School%20Work/Thesis/TropicalNN/Kurt_TropicalTests.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                     second_layer_size \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,second_layer_activation \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39msigmoid\u001b[39;49m\u001b[39m'\u001b[39;49m, training_loss \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mbinary_crossentropy\u001b[39;49m\u001b[39m'\u001b[39;49m, lam\u001b[39m=\u001b[39;49mi)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kurtp/OneDrive%20-%20Naval%20Postgraduate%20School/Desktop/NPS%20School%20Work/Thesis/TropicalNN/Kurt_TropicalTests.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m x_test_att \u001b[39m=\u001b[39m attackTestSetBatch(built_model, x_test, y_test, loss_object, \u001b[39mstr\u001b[39m(i))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kurtp/OneDrive%20-%20Naval%20Postgraduate%20School/Desktop/NPS%20School%20Work/Thesis/TropicalNN/Kurt_TropicalTests.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m pre_loss, pre_acc \u001b[39m=\u001b[39m built_model\u001b[39m.\u001b[39mevaluate(x_test, y_test)\n",
      "File \u001b[1;32mc:\\Users\\kurtp\\OneDrive - Naval Postgraduate School\\Desktop\\NPS School Work\\Thesis\\TropicalNN\\functions\\models.py:59\u001b[0m, in \u001b[0;36msimple_tropical_model\u001b[1;34m(x_train, y_train, num_epochs, first_layer_size, verbose, initializer_w, second_layer_size, second_layer_activation, clipnorm, clipvalue, training_loss, lam, boo_dropout, p_dropout)\u001b[0m\n\u001b[0;32m     56\u001b[0m     model \u001b[39m=\u001b[39m Sequential([TropEmbedMaxMin(first_layer_size, initializer_w \u001b[39m=\u001b[39m initializer_w, lam\u001b[39m=\u001b[39mlam),\n\u001b[0;32m     57\u001b[0m                     Dense(second_layer_size, activation\u001b[39m=\u001b[39msecond_layer_activation,  kernel_initializer\u001b[39m=\u001b[39minitializer_out)])\n\u001b[0;32m     58\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mAdam(\u001b[39m0.1\u001b[39m, clipnorm\u001b[39m=\u001b[39mclipnorm, clipvalue\u001b[39m=\u001b[39mclipvalue), loss\u001b[39m=\u001b[39mtraining_loss, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 59\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49mnum_epochs, verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[0;32m     60\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     61\u001b[0m elapsed_time \u001b[39m=\u001b[39m end_time \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\kurtp\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\kurtp\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\kurtp\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\kurtp\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\kurtp\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\kurtp\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\kurtp\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\kurtp\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\kurtp\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_object = losses.BinaryCrossentropy()\n",
    "x_train, x_test, y_train, y_test = load_CIFAR_data(desired_classes=[7, 8])\n",
    "tests = [0,0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2]\n",
    "for i in tests:\n",
    "    print(f'================\\n\\t{i}\\n================')\n",
    "    data = []\n",
    "    built_model = simple_tropical_model(x_train, y_train, num_epochs = 10, first_layer_size = 100, initializer_w=initializers.RandomNormal(mean=0.5, stddev=0.005, seed=0), verbose = 1,\n",
    "                        second_layer_size = 1,second_layer_activation = 'sigmoid', training_loss = 'binary_crossentropy', lam=i)\n",
    "    x_test_att = attackTestSetBatch(built_model, x_test, y_test, loss_object, str(i))\n",
    "    pre_loss, pre_acc = built_model.evaluate(x_test, y_test)\n",
    "    post_loss, post_acc = built_model.evaluate(x_test_att, y_test)\n",
    "    data = [i,pre_loss, pre_acc, post_loss, post_acc]\n",
    "    #with open('CIFAR_lambda_test2.csv', 'a', newline='') as csvfile:\n",
    "        #csvwriter = csv.writer(csvfile)\n",
    "        #csvwriter.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scat_size = 20\n",
    "bin_num = 784\n",
    "hist_wid = 10\n",
    "hist_hei = 8\n",
    "data = pd.read_csv(\"CIFAR_lambda_test.csv\")\n",
    "rowNum = data.shape[0]\n",
    "\n",
    "#lambda,pre_loss,pre_acc,post_loss,post_acc\n",
    "\n",
    "data[\"loss_diff\"] = data['post_loss'] - data['pre_loss']\n",
    "data[\"acc_diff\"] = data['post_acc'] - data['pre_acc']\n",
    "\n",
    "columns = [\"pre_loss\"]#, \"post_loss\"]\n",
    "settings = [\"Pre-attack test Loss\", \"Post-attack test Loss\"]\n",
    "\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "for i in range(len(columns)):\n",
    "    plt.scatter(data['lambda'], data[columns[i]], label=settings[i], s=scat_size)\n",
    "plt.xticks([i/100 for i in range(101)])\n",
    "#plt.yticks([i/20 for i in range(21)])\n",
    "plt.xlabel('Lambda')\n",
    "plt.xlim(-0.005, 0.205)\n",
    "plt.ylim(-1, 30)\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Loss from varying lambda for {rowNum} tests')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "for i in model_objects:\n",
    "    plt.hist(i.layers[0].get_weights()[0].flatten(), bins=bin_num, alpha=0.3, label='l')\n",
    "plt.xlabel('Initial Accuracy on Test Set')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Initial Accuracy for {rowNum} tests')\n",
    "plt.ylim(0, 7000)\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x_train, x_test, y_train, y_test = load_CIFAR_data(desired_classes=[7, 8])\n",
    "t_model1 = buildTropicalModel(x_train, y_train, verbose=1, initializer_w=BimodalNormalInitializer(stddev=1,high=5.5, low=-4.5), lam = 0)\n",
    "weights1 = t_model1.layers[0].get_weights()[0].flatten()\n",
    "\n",
    "t_model2 = buildTropicalModel(x_train, y_train,  verbose=1,initializer_w=BimodalNormalInitializer(stddev=1,high=5.5, low=-4.5), lam = 0.1)\n",
    "weights2 = t_model2.layers[0].get_weights()[0].flatten()\n",
    "\n",
    "t_model3 = buildTropicalModel(x_train, y_train, verbose=1, initializer_w=BimodalNormalInitializer(stddev=1,high=5.5, low=-4.5), lam = 0.5)\n",
    "weights3 = t_model3.layers[0].get_weights()[0].flatten()\n",
    "\n",
    "x_train, x_test, y_train, y_test = load_CIFAR_data(desired_classes=[7, 8])\n",
    "t_model1 = buildTropicalModel(x_train, y_train, verbose=1, initializer_w=initializers.RandomNormal(0.5, 0.005, seed=0), lam = 0)\n",
    "weights1 = t_model1.layers[0].get_weights()[0].flatten()\n",
    "\n",
    "t_model2 = buildTropicalModel(x_train, y_train,  verbose=1,initializer_w=initializers.RandomNormal(0.5, 0.005, seed=0), lam = 0.1)\n",
    "weights2 = t_model2.layers[0].get_weights()[0].flatten()\n",
    "\n",
    "t_model3 = buildTropicalModel(x_train, y_train, verbose=1, initializer_w=initializers.RandomNormal(0.5, 0.005, seed=0), lam = 0.5)\n",
    "weights3 = t_model3.layers[0].get_weights()[0].flatten()\n",
    "\n",
    "t_model1.evaluate(x_test, y_test)\n",
    "t_model2.evaluate(x_test, y_test)\n",
    "t_model3.evaluate(x_test, y_test)\n",
    "'''\n",
    "plt.hist(weights1, bins = 784, alpha = 0.3, label='lambda=0')\n",
    "plt.hist(weights2, bins = 784, alpha = 0.3, label='lambda=0.1')\n",
    "plt.hist(weights3, bins = 784, alpha = 0.3, label='lambda=0.5')\n",
    "plt.ylim(0,1000)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializers Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "loss_object = losses.BinaryCrossentropy()\n",
    "x_train, x_test, y_train, y_test = load_CIFAR_data()\n",
    "tests = [(buildReLuModel, initializers.RandomNormal(mean=0.5, stddev=1., seed=0), \"ReLU\"),\n",
    "         (buildTropicalModel, BimodalBinaryInitializer(high=5.5, low=-4.5), \"Tropical -4.5 or 5.5\"),\n",
    "         (buildTropicalModel, BimodalNormalInitializer(stddev=1,high=5.5, low=-4.5), \"Tropical ~N(-4.5,1) or ~N(5.5,1)\"),\n",
    "         (buildTropicalModel, Triangular(left=0,mode=0.5, right=1), \"Tropical ~Triangular(0, 0.5, 1)\"),\n",
    "         (buildTropicalModel, initializers.Constant(0.5), \"Tropical vector of all 0.5\"),\n",
    "         (buildTropicalModel, initializers.RandomNormal(0.5, 0.005, seed=0), \"Tropical ~N(0.5,0.005)\"),\n",
    "         (buildTropicalModel, initializers.random_normal, \"Tropical random_normal\")]\n",
    "\n",
    "for i in range(iterations):\n",
    "    print(f'================\\n\\t{i}\\n================')\n",
    "    x_train, x_test, y_train, y_test = shuffle_data(x_train, x_test, y_train, y_test) \n",
    "    data = []\n",
    "    for model, intializer_w, name in tests:\n",
    "        built_model = model(x_train, y_train, initializer_w=intializer_w, verbose = 1)\n",
    "        x_test_att = attackTestSetBatch(built_model, x_test, y_test, loss_object, name)\n",
    "        pre_loss, pre_acc = built_model.evaluate(x_test, y_test)\n",
    "        post_loss, post_acc = built_model.evaluate(x_test_att, y_test)\n",
    "        data.extend([pre_loss, pre_acc, post_loss, post_acc])\n",
    "    with open('Initializer_Test.csv', 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['relu', 'trop_bin', 'trop_nor', 'trop_tri', 'trop_constant', 'trop_rand_small', 'trop_rand_default']\n",
    "types = ['_pre_loss', '_pre_acc', '_post_loss', '_post_acc', '_loss_diff', '_acc_diff']\n",
    "col_headers = [i+j for i in names for j in types]\n",
    "pre_loss_headers = [i for i in col_headers if types[0] in i]\n",
    "pre_acc_headers = [i for i in col_headers if types[1] in i]\n",
    "post_loss_headers = [i for i in col_headers if types[2] in i]\n",
    "post_acc_headers = [i for i in col_headers if types[3] in i]\n",
    "loss_diff_headers = [i for i in col_headers if types[4] in i]\n",
    "acc_diff_headers = [i for i in col_headers if types[5] in i]\n",
    "settings = [tup[2] for tup in tests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scat_size = 12\n",
    "bin_num = 40\n",
    "hist_wid = 9\n",
    "hist_hei = 6\n",
    "round_dec = 4\n",
    "data = pd.read_csv(\"Initializer_Test.csv\")\n",
    "rowNum = data.shape[0]\n",
    "\n",
    "for i in range(len(settings)):\n",
    "    data[loss_diff_headers[i]] = data[post_loss_headers[i]] - data[pre_loss_headers[i]]\n",
    "    data[acc_diff_headers[i]] = data[post_acc_headers[i]] - data[pre_acc_headers[i]]\n",
    "\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "plt.boxplot(data[pre_loss_headers[1:]], labels=settings[1:])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Initialization Setting')\n",
    "plt.xlabel('Initial Loss on Test Set')\n",
    "plt.title(f'Initial Loss for {rowNum} tests')\n",
    "plt.grid(axis = 'y')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "plt.boxplot(data[pre_acc_headers[1:]], labels=settings[1:])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Initialization Setting')\n",
    "plt.xlabel('Initial Accuracy on Test Set')\n",
    "plt.title(f'Initial Accuracy for {rowNum} tests')\n",
    "plt.grid(axis = 'y')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "plt.boxplot(data[loss_diff_headers[1:]], labels=settings[1:])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Initialization Setting')\n",
    "plt.xlabel('Loss Increase After PGD Attack on Test Set')\n",
    "plt.title(f'Loss Increases for {rowNum} tests')\n",
    "plt.grid(axis = 'y')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "plt.boxplot(data[acc_diff_headers[1:]], labels=settings[1:])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Initialization Setting')\n",
    "plt.xlabel('Accuracy Decrease After PGD Attack on Test Set')\n",
    "plt.title(f'Accuracy Decreases for {rowNum} tests')\n",
    "plt.grid(axis = 'y')\n",
    "plt.show()\n",
    "\n",
    "columns1 = pre_loss_headers\n",
    "columns2 = post_loss_headers\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "for i in range(len(columns)):\n",
    "    plt.scatter(data[columns1[i]], data[columns2[i]], s=scat_size, label=settings[i])\n",
    "plt.xlabel('Pre-Attack Loss')\n",
    "plt.ylabel('Post-Attack Loss')\n",
    "plt.title(f'Loss for {rowNum} tests')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "columns1 = pre_acc_headers\n",
    "columns2 = post_acc_headers\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "for i in range(len(columns)):\n",
    "    plt.scatter(data[columns1[i]], data[columns2[i]], s=scat_size, label=settings[i])\n",
    "plt.xlabel('Pre-Attack Accuracy')\n",
    "plt.ylabel('Post-Attack Accuracy')\n",
    "plt.title(f'Accuracy for {rowNum} tests')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = load_CIFAR_data()\n",
    "tests = [(buildReLuModel, initializers.RandomNormal(mean=0.5, stddev=1., seed=0), \"ReLU\"),\n",
    "         (buildTropicalModel, BimodalBinaryInitializer(high=5.5, low=-4.5), \"Tropical -4.5 or 5.5\"),\n",
    "         (buildTropicalModel, BimodalNormalInitializer(stddev=1,high=5.5, low=-4.5), \"Tropical ~N(-4.5,1) or ~N(5.5,1)\"),\n",
    "         (buildTropicalModel, Triangular(left=0,mode=0.5, right=1), \"Tropical ~Triangular(0, 0.5, 1)\"),\n",
    "         (buildTropicalModel, initializers.Constant(0.5), \"Tropical vector of all 0.5\"),\n",
    "         (buildTropicalModel, initializers.RandomNormal(0.5, 0.005, seed=0), \"Tropical ~N(0.5,0.005)\"),\n",
    "         (buildTropicalModel, initializers.random_normal, \"Tropical random_normal\")]\n",
    "\n",
    "model_objects = []\n",
    "for model, intializer_w, name in tests:\n",
    "    built_model = model(x_train, y_train, initializer_w=intializer_w, verbose = 1)\n",
    "    model_objects.append(built_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = [tup[2] for tup in tests]\n",
    "for i in range(len(settings)):\n",
    "    t1 = model_objects[i].layers[0].get_weights()[0].flatten()\n",
    "    t1_m = np.mean(t1)\n",
    "    t1_s = np.std(t1)\n",
    "    plt.figure(figsize=(8,7))\n",
    "    plt.hist(t1, alpha=0.4,  bins = 392)\n",
    "    plt.title(f\"{settings[i]} mean={t1_m: .3f}, std={t1_s: .3f}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "t1 = model_objects[1].layers[0].get_weights()[0].flatten()\n",
    "t2 = model_objects[2].layers[0].get_weights()[0].flatten()\n",
    "\n",
    "t1_m = np.mean(t1)\n",
    "t1_s = np.std(t1)\n",
    "t2_m = np.mean(t2)\n",
    "t2_s = np.std(t2)\n",
    "\n",
    "binnies = 784\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.hist(t1, alpha=0.4, label=f\"Tropical -4.5 or 5.5 mean={t1_m: .3f}, std={t1_s: .3f}\",  bins = binnies)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.hist(t1, alpha=0.4, label=f\"Tropical -4.5 or 5.5 mean={t1_m: .3f}, std={t1_s: .3f}\",  bins = binnies)\n",
    "plt.legend()\n",
    "plt.ylim(0, 5000)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "initializer = BimodalNormalInitializer(stddev=1,high=5.5, low=-4.5) \n",
    "weights_tensor = initializer(shape=(100, 3072), dtype=tf.float32)\n",
    "plt.hist(weights_tensor.numpy().flatten(), alpha=0.4,bins=binnies, label='Initial Weights ~N(-4.5,1) or ~N(5.5,1)')\n",
    "plt.hist(t2, alpha=0.4, label=f\"Post-Training ~N(-4.5,1) or ~N(5.5,1) mean={t2_m: .3f}, std={t2_s: .3f}\", bins = binnies)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST, all numbers PGD test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 50\n",
    "loss_object = losses.CategoricalCrossentropy()\n",
    "x_train, x_test, y_train, y_test = load_MNIST_data(desired_classes=[i for i in range(10)])\n",
    "#tests = [(buildReLuModel, initializers.RandomNormal(mean=0.5, stddev=1., seed=0), \"ReLU\"),\n",
    "#         (buildTropicalModel, BimodalBinaryInitializer(high=4, low=-3), \"Tropical -3 or 4\"),\n",
    "tests = [(buildTropicalModel, BimodalNormalInitializer(stddev=1,high=4, low=-3), \"Tropical ~N(-3,1) or ~N(4,1)\")]\n",
    "#         (buildTropicalModel, initializers.RandomNormal(0.5, 0.005, seed=0), \"Tropical ~N(0.5,0.005)\")]\n",
    "\n",
    "model_objects = []\n",
    "for i in range(iterations):\n",
    "    print(f'================\\n\\t{i}\\n================')\n",
    "    x_train, x_test, y_train, y_test = shuffle_data(x_train, x_test, y_train, y_test) \n",
    "    data = []\n",
    "    \n",
    "    for model, intializer_w, name in tests:\n",
    "        built_model = model(x_train, y_train, first_layer_size = 100, initializer_w=intializer_w, verbose = 1,\n",
    "                            second_layer_size = 10,second_layer_activation = 'softmax', training_loss = 'categorical_crossentropy')\n",
    "        #x_test_att = attackTestSetBatch(built_model, x_test, y_test, loss_object, name)\n",
    "        #pre_loss, pre_acc = built_model.evaluate(x_test, y_test)\n",
    "        #post_loss, post_acc = built_model.evaluate(x_test_att, y_test)\n",
    "        #data.extend([pre_loss, pre_acc, post_loss, post_acc])\n",
    "        model_objects.append(built_model)\n",
    "    #with open('bimodalTest.csv', 'a', newline='') as csvfile:\n",
    "        #csvwriter = csv.writer(csvfile)\n",
    "        #csvwriter.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "save_dir = 'saved_models'  # Change this to your desired directory\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "for i, model in enumerate(model_objects):\n",
    "    model_name = f'model_{i}.h5'  # Adjust the naming scheme as needed\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    tf.keras.models.save_model(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scat_size = 12\n",
    "bin_num = 30\n",
    "hist_wid = 14\n",
    "hist_hei = 12\n",
    "data = pd.read_csv(\"bimodalTest.csv\")\n",
    "rowNum = data.shape[0]\n",
    "\n",
    "data[\"relu_loss_diff\"] = data['relu_post_loss'] - data['relu_pre_loss']\n",
    "data[\"relu_acc_diff\"] = data['relu_post_acc'] - data['relu_pre_acc']\n",
    "\n",
    "data[\"trop_loss_diff1\"] = data['trop_post_loss1'] - data['trop_pre_loss1']\n",
    "data[\"trop_acc_diff1\"] = data['trop_post_acc1'] - data['trop_pre_acc1']\n",
    "\n",
    "data[\"trop_loss_diff2\"] = data['trop_post_loss2'] - data['trop_pre_loss2']\n",
    "data[\"trop_acc_diff2\"] = data['trop_post_acc2'] - data['trop_pre_acc2']\n",
    "\n",
    "data[\"trop_loss_diff3\"] = data['trop_post_loss3'] - data['trop_pre_loss3']\n",
    "data[\"trop_acc_diff3\"] = data['trop_post_acc3'] - data['trop_pre_acc3']\n",
    "\n",
    "columns = [\"relu_pre_loss\", \"trop_pre_loss1\", \"trop_pre_loss2\", \"trop_pre_loss3\"]\n",
    "settings = [\"ReLU\", \"Tropical -3 or 4\", \"Tropical ~N(-3,1) or ~N(4,1)\", \"Tropical ~N(0.5,0.005)\"]\n",
    "\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "for i in range(len(columns)):\n",
    "    plt.hist(np.array(data[columns[i]]), bins=bin_num, alpha=0.5, label=f'{settings[i]} Loss. Mean={round(data[columns[i]].mean(),2)}, Std={round(data[columns[i]].std(),2)}')\n",
    "plt.xlabel('Initial Loss on Test Set')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Initial Loss for {rowNum} tests')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "columns = [\"relu_pre_acc\", \"trop_pre_acc1\", \"trop_pre_acc2\", \"trop_pre_acc3\"]\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "for i in range(len(columns)):\n",
    "    plt.hist(np.array(data[columns[i]]), bins=bin_num, alpha=0.5, label=f'{settings[i]} Accuracy. Mean={round(data[columns[i]].mean(),2)}, Std={round(data[columns[i]].std(),2)}')\n",
    "plt.xlabel('Initial Accuracy on Test Set')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Initial Accuracy for {rowNum} tests')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "columns = [\"relu_loss_diff\", \"trop_loss_diff1\", \"trop_loss_diff2\", \"trop_loss_diff3\"]\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "for i in range(len(columns)):\n",
    "    plt.hist(np.array(data[columns[i]]), bins=bin_num, alpha=0.5, label=f'{settings[i]} Loss Increase. Mean={round(data[columns[i]].mean(),2)}, Std={round(data[columns[i]].std(),2)}')\n",
    "plt.xlabel('Loss Increase After PGD Attack on Test Set')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Loss Increases for {rowNum} tests')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "columns = [\"relu_acc_diff\", \"trop_acc_diff1\", \"trop_acc_diff2\", \"trop_acc_diff3\"]\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "for i in range(len(columns)):\n",
    "    plt.hist(np.array(data[columns[i]]), bins=bin_num, alpha=0.5, label=f'{settings[i]} Accuracy Increase. Mean={round(data[columns[i]].mean(),2)}, Std={round(data[columns[i]].std(),2)}')\n",
    "plt.xlabel('Accuracy Decrease After PGD Attack on Test Set')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Accuracy Decreases for {rowNum} tests')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "columns1 = [\"relu_pre_loss\", \"trop_pre_loss1\", \"trop_pre_loss2\", \"trop_pre_loss3\"]\n",
    "columns2 = [\"relu_post_loss\", \"trop_post_loss1\", \"trop_post_loss2\", \"trop_post_loss3\"]\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "for i in range(len(columns)):\n",
    "    plt.scatter(data[columns1[i]], data[columns2[i]], s=scat_size, label=settings[i])\n",
    "plt.xlabel('Pre-Attack Loss')\n",
    "plt.ylabel('Post-Attack Loss')\n",
    "plt.title(f'Loss for {rowNum} tests')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "columns1 = [\"relu_pre_acc\", \"trop_pre_acc1\", \"trop_pre_acc2\", \"trop_pre_acc3\"]\n",
    "columns2 = [\"relu_post_acc\", \"trop_post_acc1\", \"trop_post_acc2\", \"trop_post_acc3\"]\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "for i in range(len(columns)):\n",
    "    plt.scatter(data[columns1[i]], data[columns2[i]], s=scat_size, label=settings[i])\n",
    "plt.xlabel('Pre-Attack Accuracy')\n",
    "plt.ylabel('Post-Attack Accuracy')\n",
    "plt.title(f'Accuracy for {rowNum} tests')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_images = x_test.reshape(-1, 28, 28)\n",
    "relu_images = relu_x_test_att.reshape(-1, 28, 28)\n",
    "trop_images = trop_x_test_att1.reshape(-1, 28, 28)\n",
    "\n",
    "def show_images(images, num_images=5):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(images[i+5], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(normal_images, num_images=5)\n",
    "show_images(relu_images, num_images=5)\n",
    "show_images(trop_images, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_predictions = [model_objects[i].predict(x_test) for i in range(len(model_objects))]\n",
    "ensemble_sum = np.sum(model_predictions, axis=0)\n",
    "true_class_indices = np.argmax(y_test, axis=1)\n",
    "ensemble_class_indices = np.argmax(ensemble_sum, axis=1)\n",
    "accuracy = np.mean(true_class_indices == ensemble_class_indices)\n",
    "print(f'Ensemble Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    model_objects[i].evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluModel = model_objects[0]\n",
    "weights = reluModel.layers[0].get_weights()[0].flatten()\n",
    "biases = reluModel.layers[0].get_weights()[1].flatten()\n",
    "print(weights.shape)\n",
    "plt.hist(weights, bins=int(784), alpha=0.5)\n",
    "#plt.hist(biases, bins=int(784), alpha=0.5)\n",
    "plt.ylim(0, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = model_objects[1].layers[0].get_weights()[0].flatten()\n",
    "t2 = model_objects[2].layers[0].get_weights()[0].flatten()\n",
    "t3 = model_objects[3].layers[0].get_weights()[0].flatten()\n",
    "\n",
    "t1_m = np.mean(t1)\n",
    "t1_s = np.std(t1)\n",
    "t2_m = np.mean(t2)\n",
    "t2_s = np.std(t2)\n",
    "t3_m = np.mean(t3)\n",
    "t3_s = np.std(t3)\n",
    "\n",
    "binnies = 784\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.hist(t1, alpha=0.4, label=f\"bernoulli mean={t1_m: .3f}, std={t1_s: .3f}\",  bins = binnies)\n",
    "plt.hist(t2, alpha=0.4, label=f\"bernoulli normals mean={t2_m: .3f}, std={t2_s: .3f}\", bins = binnies)\n",
    "plt.hist(t3, alpha=0.4, label=f\"random_normal mean={t3_m: .3f}, std={t3_s: .3f}\", bins = binnies)\n",
    "#plt.hist(weights, alpha=0.4, label=\"ReLU\", density=True, bins = binnies)\n",
    "plt.ylim(0, 600)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = Triangular(left=0,mode=0.5, right=1)  # You can set the seed for reproducibility\n",
    "weights_tensor = initializer(shape=(100, 784), dtype=tf.float32)\n",
    "plt.hist(weights_tensor.numpy().flatten(), bins=784)\n",
    "plt.hist(t3, bins=784, alpha=0.4, label=f\"random_normal {round(np.mean(t3),3)}, {np.sum(t3 < 0)}, {np.sum(t3 > 0)}\")\n",
    "plt.ylim(0, 600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = BimodalNormalInitializer(high=4, low=-3, seed=42)  # You can set the seed for reproducibility\n",
    "weights_tensor = initializer(shape=(500, 784), dtype=tf.float32)\n",
    "#weights_tensor.numpy()\n",
    "plt.hist(weights_tensor.numpy().flatten(), bins = 784,alpha =0.4)\n",
    "plt.hist(built_model.layers[0].get_weights()[0].flatten(), bins=int(784), alpha=0.4)\n",
    "plt.ylim(0, 3000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PGD Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 500 #aribitrary\n",
    "loss_object = losses.BinaryCrossentropy()\n",
    "x_train, x_test, y_train, y_test = load_MNIST_data(desired_classes=[2, 8])\n",
    "\n",
    "for i in range(iterations):\n",
    "    print(f'================\\n\\t{i}\\n================')\n",
    "    x_train, x_test, y_train, y_test = shuffle_data(x_train, x_test, y_train, y_test) \n",
    "\n",
    "    reluModel = buildReLuModel(x_train, y_train, verbose=1)\n",
    "    #relu_x_test_att = attackTestSet(reluModel, x_test, y_test, loss_object, \"ReLu\")\n",
    "    relu_pre_loss, relu_pre_acc = reluModel.evaluate(x_test, y_test)\n",
    "    #relu_post_loss, relu_post_acc = reluModel.evaluate(relu_x_test_att, y_test)\n",
    "\n",
    "    tropModel1 = buildTropicalModel(x_train, y_train, verbose=1)\n",
    "    #trop_x_test_att1 = attackTestSet(tropModel1, x_test, y_test, loss_object, \"Tropical\")\n",
    "    trop_pre_loss1, trop_pre_acc1 = tropModel1.evaluate(x_test, y_test)\n",
    "    #trop_post_loss1, trop_post_acc1 = tropModel1.evaluate(trop_x_test_att1, y_test)\n",
    "\n",
    "    # - write to csv - \n",
    "    data = [relu_pre_loss, relu_pre_acc, relu_post_loss, relu_post_acc,trop_pre_loss1, trop_pre_acc1, trop_post_loss1, trop_post_acc1]\n",
    "    with open('output4.csv', 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scat_size = 8\n",
    "bin_num = 4\n",
    "hist_wid = 8\n",
    "hist_hei = 6\n",
    "data = pd.read_csv(\"output3.csv\")\n",
    "rowNum = data.shape[0]\n",
    "\n",
    "data[\"relu_loss_diff\"] = data['relu_post_loss'] - data['relu_pre_loss']\n",
    "data[\"trop_loss_diff1\"] = data['trop_post_loss1'] - data['trop_pre_loss1']\n",
    "data[\"relu_acc_diff\"] = data['relu_post_acc'] - data['relu_pre_acc']\n",
    "data[\"trop_acc_diff1\"] = data['trop_post_acc1'] - data['trop_pre_acc1']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "plt.hist(np.array(data[\"relu_pre_loss\"]), bins=bin_num, alpha=0.5, label=f'ReLu Loss. Mean={round(data[\"relu_pre_loss\"].mean(),2)}, Std={round(data[\"relu_pre_loss\"].std(),2)}')\n",
    "plt.hist(np.array(data[\"trop_pre_loss1\"]), bins=bin_num, alpha=0.5, label=f'Tropical (initial_w = random_normal) Loss. Mean={round(data[\"trop_pre_loss1\"].mean(),2)}, Std={round(data[\"trop_pre_loss1\"].std(),2)}')\n",
    "plt.xlabel('Initial Loss on Test Set')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Initial Loss for {rowNum} tests')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "plt.hist(np.array(data[\"relu_pre_acc\"]), bins=bin_num, alpha=0.5, label=f'ReLu Accuracy. Mean={round(data[\"relu_pre_acc\"].mean(),2)}, Std={round(data[\"relu_pre_acc\"].std(),2)}')\n",
    "plt.hist(np.array(data[\"trop_pre_acc1\"]), bins=bin_num, alpha=0.5, label=f'Tropical (initial_w = random_normal) Accuracy. Mean={round(data[\"trop_pre_acc1\"].mean(),2)}, Std={round(data[\"trop_pre_acc1\"].std(),2)}')\n",
    "plt.xlabel('Initial Accuracy on Test Set')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Initial Accuracy for {rowNum} tests')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "plt.hist(np.array(data[\"relu_loss_diff\"]), bins=bin_num, alpha=0.5, label=f'ReLU Loss Increase. Mean={round(data[\"relu_loss_diff\"].mean(),2)}, Std={round(data[\"relu_loss_diff\"].std(),2)}')\n",
    "plt.hist(np.array(data[\"trop_loss_diff1\"]), bins=bin_num, alpha=0.5, label=f'Tropical (initial_w = random_normal) Loss Increase. Mean={round(data[\"trop_loss_diff1\"].mean(),2)}, Std={round(data[\"trop_loss_diff1\"].std(),2)}')\n",
    "plt.xlabel('Loss Increase After PGD Attack on Test Set')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Loss Increases for {rowNum} tests')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "plt.hist(np.array(data[\"relu_acc_diff\"]), bins=bin_num, alpha=0.5, label=f'ReLU Accuracy Decrease. Mean={round(data[\"relu_acc_diff\"].mean(),2)}, Std={round(data[\"relu_acc_diff\"].std(),2)}')\n",
    "plt.hist(np.array(data[\"trop_acc_diff1\"]), bins=bin_num, alpha=0.5, label=f'Tropical (initial_w = random_normal) Accuracy Decrease. Mean={round(data[\"trop_acc_diff1\"].mean(),2)}, Std={round(data[\"trop_acc_diff1\"].std(),2)}')\n",
    "plt.xlabel('Accuracy Decrease After PGD Attack on Test Set')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Accuracy Decreases for {rowNum} tests')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "plt.scatter(data['relu_pre_loss'], data['relu_post_loss'], s=6, label=f'ReLU')\n",
    "plt.scatter(data['trop_pre_loss1'], data['trop_post_loss1'], s=scat_size, label=f'Tropical (initial_w = random_normal)')\n",
    "plt.xlabel('Pre-Attack Loss')\n",
    "plt.ylabel('Post-Attack Loss')\n",
    "plt.title(f'Loss for {rowNum} tests')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(hist_wid,hist_hei))\n",
    "plt.scatter(data['relu_pre_acc'], data['relu_post_acc'], s=6, label=f'ReLU')\n",
    "plt.scatter(data['trop_pre_acc1'], data['trop_post_acc1'], s=scat_size, label=f'Tropical (initial_w = random_normal)')\n",
    "plt.xlabel('Pre-Attack Accuracy')\n",
    "plt.ylabel('Post-Attack Accuracy')\n",
    "plt.title(f'Accuracy for {rowNum} tests')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
